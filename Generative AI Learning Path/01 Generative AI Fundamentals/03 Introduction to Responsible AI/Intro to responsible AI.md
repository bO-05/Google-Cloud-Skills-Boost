# Introduction to Responsible AI

This course will help you understand Google's AI principles and practices. You'll learn why responsible AI is important and how your decisions impact it. You will:
- Understand why Google has put AI principles in place. 
- Identify the need for a responsible AI practice within an organization. 
- Recognize that decisions made at all stages of a project have an impact on responsible AI. 
- Recognize that organizations can design AI to fit their own business needs and values.

## What is responsible AI?

Responsible AI considers the possible issues, limitations or unintended consequences of AI and aims to develop it safely and for the benefit of all. There is no universal definition or checklist for responsible AI. Organizations develop their own principles reflecting their mission and values. Common themes are transparency, fairness, accountability and privacy.

## Google's approach

Google's approach to responsible AI is to build AI that is:

- For everyone 
- Accountable and safe
- Respects privacy
- Scientifically excellent

We incorporate responsibility by design into our products and organization. Our AI principles guide responsible decision making. Whatever your role, your decisions impact responsible AI.  

## The role of people 

People, not machines, make the key decisions in AI development:

- Collecting or creating data 
- Controlling deployment and application
- Introducing their values at each decision point

Even with good intent, AI could cause issues or unintended outcomes without responsible practices. Responsible AI leads to better, more trustworthy models. Broken trust risks unsuccessful or harmful AI.

## Assessing and reviewing AI

Google makes product and business decisions through assessments and reviews ensuring our AI principles are met. Though people may disagree, robust processes build trust in the end decision.

## Google's AI principles

We announced seven AI principles in June 2018 to govern our work:

1. AI should be socially beneficial. We proceed only if benefits outweigh risks. 
2. AI should avoid unfair bias. We avoid unjust impacts, especially on sensitive groups.
3. AI should be safe and secure. We apply safety practices to avoid harm.
4. AI should be accountable. We provide opportunities for feedback, explanations and appeal. 
5. AI should incorporate privacy. We give notice, consent and control over data use.  
6. AI should uphold scientific excellence. We promote rigorous, multidisciplinary leadership and share knowledge.
7. AI should accord with these principles. We limit harmful or abusive applications.

We won't pursue AI for:

- Overall harm 
- Weapons/injury 
- Surveillance violating norms 
- Violating law/human rights

Our principles guide but don't directly answer questions. They establish what we stand for and build, enabling hard conversations.
